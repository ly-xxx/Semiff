#!/usr/bin/env python3
"""
Semiff Main Entry: Real-to-Sim-to-Real Pipeline
"""
import hydra
from pathlib import Path
import numpy as np
from omegaconf import DictConfig

# å¼•å…¥æˆ‘ä»¬å®šä¹‰çš„æ¨¡å—
from semiff.core.io import VideoReader, RobotLogger
from semiff.perception.mast3r_wrapper import MASt3RWrapper
from semiff.perception.sam2_wrapper import SAM2Wrapper
from semiff.calibration.robot_aligner import align_visual_to_robot
from semiff.calibration.space_trans import RigidTransform, apply_transform_to_cameras
from semiff.geometry.meshing import Mesher
from semiff.geometry.decomposition import ColliderBuilder
from semiff.rendering.dataset_prep import NerfstudioConverter, estimate_intrinsics
from semiff.core.logger import get_logger

logger = get_logger("semiff_main")

@hydra.main(config_path="src/semiff/config", config_name="defaults", version_base=None)
def main(cfg: DictConfig):
    logger.info("ğŸš€ Starting Semiff Pipeline...")
    output_dir = Path(cfg.data.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # === Stage 1: Data Ingestion ===
    logger.info(">>> [Stage 1] Loading Data")
    # åŠ è½½è§†é¢‘
    with VideoReader(cfg.data.video_path) as video:
        frames, timestamps = video.get_frames(step=cfg.pipeline.keyframe_interval)
        # ä¿å­˜è§†é¢‘å…ƒæ•°æ®ä»¥ä¾›åç»­ä½¿ç”¨
        video_meta = {'width': video.width, 'height': video.height, 'fps': video.fps}

    # åŠ è½½æœºå™¨äººæ—¥å¿— (å¯é€‰ï¼Œç”¨äºå¯¹é½)
    if cfg.data.robot_logs:
        robot_log = RobotLogger(cfg.data.robot_logs)
        # è·å–ä¸è§†é¢‘å¸§å¯¹åº”çš„å…³èŠ‚è§’åº¦
        joints = robot_log.get_interpolated_joints(timestamps)
        logger.info(f"Aligned {len(joints)} joint states to video frames.")

    # === Stage 2: Perception (Geometry) ===
    logger.info(">>> [Stage 2] Running MASt3R for Sparse Reconstruction")
    mast3r = MASt3RWrapper(device=cfg.device)
    # è¿è¡Œ MASt3R å¾—åˆ°ä½å§¿å’Œç‚¹äº‘
    poses, scene_cloud = mast3r.run(frames)

    # ä¿å­˜ä¸­é—´ç»“æœ
    np.save(output_dir / "poses.npy", poses)
    # TODO: ä¿å­˜ scene_cloud ä¸º .ply (éœ€è¦ open3d)

    # === Stage 3: Perception (Semantics) ===
    logger.info(">>> [Stage 3] Running SAM 2 for Segmentation")
    sam2 = SAM2Wrapper(cfg)
    mask_paths = sam2.run(
        video_path=cfg.data.video_path,
        output_dir=output_dir,
        scene_cloud=scene_cloud # ä¼ å…¥ç‚¹äº‘ç”¨äºè¾…åŠ© Prompting
    )

    logger.info("âœ… Perception stages completed.")

    # === Stage 4: Calibration (Sim2Real Alignment) ===
    logger.info(">>> [Stage 4] Aligning Coordinate Systems...")

    # å‡è®¾æˆ‘ä»¬é€‰æ‹©è§†é¢‘çš„ç¬¬ 10 å¸§ä½œä¸ºå¯¹é½å‚è€ƒå¸§ (æ­¤æ—¶æœºå™¨äººå§¿æ€è¾ƒå¥½)
    ref_frame_idx = min(10, len(timestamps) - 1)  # ç¡®ä¿ç´¢å¼•ä¸è¶Šç•Œ
    ref_time = timestamps[ref_frame_idx]

    # åˆ†æ”¯é€»è¾‘ï¼šMode 1 (æœ‰æ—¥å¿—) vs Mode 2 (æ— æ—¥å¿—)
    if cfg.data.robot_logs:
        logger.info("Mode 1: Log-based Hard Alignment")
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬éœ€è¦ä»…ä»…æå–å±äºæœºå™¨äººçš„ç‚¹äº‘
        # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œä½ éœ€è¦åˆ©ç”¨ SAM2 çš„ mask (mask_paths) æ¥ç´¢å¼• scene_cloud
        # è¿™é‡Œæš‚æ—¶ä¼ å…¥å®Œæ•´çš„ cloud (å‡è®¾åœºæ™¯ä¸»è¦æ˜¯æœºå™¨äººï¼Œæˆ–è€…ä¾é  ICP çš„é²æ£’æ€§)
        T_align = align_visual_to_robot(
            visual_cloud=scene_cloud, # TODO: Filter this with robot mask!
            robot_mask=None,
            robot_urdf=cfg.data.robot_urdf,
            robot_logs=cfg.data.robot_logs,
            timestamp=ref_time
        )
    else:
        logger.info("Mode 2: Visual-only Alignment (No Logs)")
        from semiff.calibration.solver import RobotOptimizer

        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        solver = RobotOptimizer(urdf_path=cfg.data.robot_urdf, device=cfg.device)

        # è¿è¡Œä¼˜åŒ– (åç®—å…³èŠ‚è§’ + åŸºåº§å˜æ¢)
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä¼ å…¥è¿™ä¸€å¸§çš„è§†è§‰ç‚¹äº‘
        # ä¸”éœ€è¦å¤„ç† mask è¿‡æ»¤ (ç›®å‰ solver.optimize æ¥æ”¶çš„æ˜¯ target_cloud)
        best_q, T_base = solver.optimize(target_cloud=scene_cloud)

        # å°† T_base è½¬ä¸º RigidTransform
        T_align = RigidTransform(T_base)

    # ä¿å­˜å˜æ¢çŸ©é˜µ
    np.save(output_dir / "T_world.npy", T_align.matrix)
    logger.info(f"Alignment Transform:\n{T_align}")

    # åº”ç”¨å˜æ¢åˆ°æ‰€æœ‰ç›¸æœºä½å§¿ (è½¬åˆ° URDF åæ ‡ç³»)
    aligned_poses = apply_transform_to_cameras(poses, T_align)
    np.save(output_dir / "poses_aligned.npy", aligned_poses)

    logger.info("âœ… Calibration completed.")

    # === Stage 5: Geometry Asset Generation ===
    logger.info(">>> [Stage 5] Generating Physics Assets...")

    # 1. è¿‡æ»¤ç‚¹äº‘ï¼šåªä¿ç•™ç‰©ä½“
    # åœ¨ç”Ÿäº§çº§ä»£ç ä¸­ï¼Œè¿™é‡Œåº”è¯¥ä½¿ç”¨ 3D Mask æˆ– æŠ•å½± 2D Mask æ¥è¿‡æ»¤
    # è¿™é‡Œåšä¸ªç®€å•çš„ Mockï¼Œå‡è®¾ aligned_cloud å·²ç»è¢«è£å‰ªæˆ–è€…æ˜¯ç‰©ä½“çš„
    # å®é™…ï¼šmesher.clean_cloud(scene_cloud, mask_indices=...)

    mesher = Mesher()
    mesh_path = mesher.run(scene_cloud, output_path=output_dir / "assets" / "object_raw.obj")

    collider = ColliderBuilder()
    collision_path = collider.decompose(mesh_path, output_path=output_dir / "assets" / "object_collision.obj")

    # === Stage 6: Rendering Dataset Prep ===
    logger.info(">>> [Stage 6] Preparing Background Training Data...")

    # ä¼°ç®—å†…å‚ (å› ä¸º MASt3R Wrapper ç›®å‰æ²¡æœ‰è¿”å›ç²¾ç¡®å†…å‚)
    intrinsics = estimate_intrinsics(video_meta['width'], video_meta['height']) # ä» VideoReader è·å–

    ns_converter = NerfstudioConverter(output_dir=output_dir / "nerfstudio")
    ns_converter.process(
        frames=frames,
        masks_dir=mask_paths['object_masks'], # æ¥è‡ª SAM2
        poses=aligned_poses,                  # æ¥è‡ª Calibration
        intrinsics=intrinsics
    )

    logger.info("âœ… All assets prepared. Ready for Warp Simulation & Splatfacto training.")
    logger.info(f"Check outputs in {output_dir}")

if __name__ == "__main__":
    main()