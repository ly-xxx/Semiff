import json
import torch
import numpy as np
import sys
import logging
import cv2
import warnings
from pathlib import Path
from typing import List, Tuple, Optional, Any, Dict

warnings.filterwarnings("ignore")
logger = logging.getLogger(__name__)

# ==================== 1. è·¯å¾„ä¸Žå…¨å±€å˜é‡å®šä¹‰ (ä¿®å¤ç‚¹) ====================
# å¿…é¡»åœ¨è¿™é‡Œå®šä¹‰ PROJECT_ROOTï¼Œä»¥ä¾¿ç±»å†…éƒ¨å¯ä»¥è®¿é—®
CURRENT_FILE = Path(__file__).resolve()
# å‡è®¾ç»“æž„æ˜¯ src/semiff/perception/mast3r_wrapper.py
# parents[0]=perception, [1]=semiff(pkg), [2]=src, [3]=ProjectRoot
PROJECT_ROOT = CURRENT_FILE.parents[3] 

MAST3R_ROOT = PROJECT_ROOT / "third_party" / "mast3r"
DUST3R_ROOT = MAST3R_ROOT / "dust3r"

# æ·»åŠ åˆ°ç³»ç»Ÿè·¯å¾„ä»¥ä¾¿ import
if DUST3R_ROOT.exists() and str(DUST3R_ROOT) not in sys.path:
    sys.path.insert(0, str(DUST3R_ROOT))
if MAST3R_ROOT.exists() and str(MAST3R_ROOT) not in sys.path:
    sys.path.insert(0, str(MAST3R_ROOT))

# ==================== 2. Import æ¨¡åž‹ ====================
try:
    from dust3r.inference import inference
    from dust3r.image_pairs import make_pairs
    from dust3r.cloud_opt import global_aligner, GlobalAlignerMode
    from mast3r.model import AsymmetricMASt3R
except ImportError as e:
    logger.error(f"âŒ Critical Import Error: {e}")
    inference = make_pairs = global_aligner = GlobalAlignerMode = AsymmetricMASt3R = None

# ==================== 3. è¾…åŠ©ç±» ====================
class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, (np.int_, np.intc, np.intp, np.int8,
                            np.int16, np.int32, np.int64, np.uint8,
                            np.uint16, np.uint32, np.uint64)):
            return int(obj)
        elif isinstance(obj, (np.float_, np.float16, np.float32, np.float64)):
            return float(obj)
        elif isinstance(obj, (np.ndarray,)):
            return obj.tolist()
        return json.JSONEncoder.default(self, obj)

# ==================== 4. ä¸»ç±»å®šä¹‰ ====================
class MASt3RWrapper:
    def __init__(self, device: str = "cuda"):
        self.device = device
        self.model = self._load_model()
        
    def _load_model(self):
        # è¿™é‡Œä½¿ç”¨äº†å…¨å±€å˜é‡ PROJECT_ROOT
        model_path = PROJECT_ROOT / "checkpoints" / "MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth"
        
        if not model_path.exists():
            logger.error(f"âŒ Model not found at: {model_path}")
            return None
            
        logger.info(f"... loading model from {model_path}")
        try:
            model = AsymmetricMASt3R.from_pretrained(str(model_path)).to(self.device)
            model.eval()
            return model
        except Exception as e:
            logger.error(f"âŒ Failed to load model: {e}")
            return None

    def run(self, frames: List[np.ndarray], keyframe_interval: int = 2, debug_dir: Optional[Path] = None) -> Tuple[List[np.ndarray], np.ndarray]:
        """
        è¿è¡Œ MASt3R é‡å»ºæµç¨‹ï¼ŒåŒ…å«æ˜¾å­˜ä¼˜åŒ–çš„ Global Alignment
        """
        if self.model is None: 
            logger.error("âŒ Model is not loaded, skipping execution.")
            return [], np.array([])
        
        # --- æ˜¾å­˜æ¸…ç†ï¼šå¼€å§‹å‰å…ˆæ¸…ç† ---
        torch.cuda.empty_cache()

        # --- 1. å‡†å¤‡å›¾åƒ ---
        raw_h, raw_w = frames[0].shape[:2]
        MODEL_LONG_EDGE = 512.0
        scale = MODEL_LONG_EDGE / max(raw_h, raw_w)
        target_h = (int(raw_h * scale) // 16) * 16
        target_w = (int(raw_w * scale) // 16) * 16

        images = []
        # é™åˆ¶æœ€å¤§å¸§æ•°ï¼Œä¿æŠ¤æ˜¾å­˜ (RTX 3090/4090 å»ºè®® 40-50 å¸§)
        MAX_FRAMES = 42 
        key_indices = list(range(0, len(frames), keyframe_interval))
        
        if len(key_indices) > MAX_FRAMES:
            logger.warning(f"âš ï¸ Limiting frames from {len(key_indices)} to {MAX_FRAMES} for memory safety.")
            key_indices = np.linspace(0, len(frames)-1, MAX_FRAMES, dtype=int).tolist()

        logger.info(f"Preparing {len(key_indices)} frames for Global Alignment...")

        for i, idx in enumerate(key_indices):
            img_tensor = self._preprocess_image_strict(frames[idx], target_h, target_w)
            images.append({
                'img': img_tensor,
                'idx': i,
                # æ ‡å‡†åŒ– true_shape æ ¼å¼ä¸º tensor
                'true_shape': torch.tensor([[target_h, target_w]], dtype=torch.long), 
                'instance': str(i)
            })

        if not images: return [], np.array([])

        # --- 2. æž„å»º Pair ---
        # swin-2 æ„å‘³ç€æ¯ä¸ªèŠ‚ç‚¹è¿žæŽ¥ 2 å±‚çš„é‚»å±…ï¼Œæ¯”å…¨è¿žæŽ¥çœå†…å­˜
        pairs = make_pairs(images, scene_graph="swin-2", prefilter=None, symmetrize=True)

        logger.info(f"ðŸš€ Running Inference on {len(pairs)} pairs...")

        # --- 3. Inference & Global Alignment ---
        # è¿™ä¸€æ­¥äº§ç”Ÿå¤§é‡ä¸­é—´æ•°æ®
        output = inference(pairs, self.model, self.device, batch_size=1, verbose=False)

        # åˆå§‹åŒ– GlobalAligner
        mode = GlobalAlignerMode.PointCloudOptimizer if len(images) > 2 else GlobalAlignerMode.PairViewer
        scene = global_aligner(output, device=self.device, mode=mode, verbose=False)

        # ðŸ”¥ã€å…³é”®å†…å­˜ä¼˜åŒ–ã€‘ï¼šscene åˆå§‹åŒ–åŽï¼Œoutput ä¸­çš„ heavy data å·²ç»è¢« scene æŽ¥ç®¡æˆ–ä¸å†éœ€è¦
        # å¿…é¡»æ˜¾å¼åˆ é™¤ output å¹¶æ¸…ç©ºç¼“å­˜ï¼Œå¦åˆ™æ˜¾å­˜ä¼šåŒå€å ç”¨ï¼Œæžæ˜“ OOM
        del output
        del pairs
        torch.cuda.empty_cache()

        if mode == GlobalAlignerMode.PointCloudOptimizer:
            scene.compute_global_alignment(
                init="mst", 
                niter=300, 
                schedule='linear', 
                lr=0.01
            )

        # --- 4. æå–ç»“æžœ ---
        # è®¾å®šç½®ä¿¡åº¦é˜ˆå€¼ï¼Œè¿‡æ»¤æŽ‰ä¸å¯é çš„ç‚¹ï¼ˆæ¯”å¦‚å¤©ç©ºã€åå…‰åŒºåŸŸï¼‰
        scene.min_conf_thr = 5.0 
        
        # è¿™ä¸€æ­¥ä¼šæ ¹æ® min_conf_thr è¿›è¡Œè£å‰ª
        scene = scene.clean_pointcloud()

        # æå–ä½å§¿ (World_T_Camera)
        refined_poses = [p.detach().cpu().numpy() for p in scene.get_im_poses()]
        
        # æå–ç‚¹äº‘
        # GlobalAligner å·²ç»æŠŠæ‰€æœ‰ç‚¹éƒ½è½¬åˆ°äº† World åæ ‡ç³»
        pts_tensor = scene.get_pts3d() # [N_imgs, H, W, 3]
        
        all_pts = []
        all_cols = []
        
        # åªéœ€è¦ç”¨æ¥å–é¢œè‰²çš„ raw tensor
        imgs_tensors = [d['img'] for d in images]

        for i in range(len(images)):
            # èŽ·å–åæ ‡
            pts_np = pts_tensor[i].detach().cpu().numpy().reshape(-1, 3)
            
            # èŽ·å–é¢œè‰²
            rgb_np = imgs_tensors[i].squeeze(0).permute(1, 2, 0).cpu().numpy().reshape(-1, 3)
            rgb_u8 = (rgb_np * 255).astype(np.uint8)

            # è¿‡æ»¤é€»è¾‘ï¼šGlobalAligner ä¼šæŠŠè¢«è¿‡æ»¤çš„ç‚¹è®¾ä¸º 0 æˆ– inf
            # æˆ‘ä»¬åªéœ€è¦ä¿ç•™éžé›¶ä¸”æœ‰æ•ˆçš„ç‚¹
            norm = np.linalg.norm(pts_np, axis=1)
            valid = (norm > 1e-6) & (np.isfinite(pts_np).all(axis=1))
            
            p_valid = pts_np[valid]
            c_valid = rgb_u8[valid]

            # é™é‡‡æ ·ï¼šæ¯å¼ å›¾æœ€å¤šè´¡çŒ® 2w ä¸ªç‚¹ï¼Œé˜²æ­¢æ€»ç‚¹äº‘è¿‡å¤§
            if p_valid.shape[0] > 20000:
                choice = np.random.choice(p_valid.shape[0], 20000, replace=False)
                p_valid = p_valid[choice]
                c_valid = c_valid[choice]

            all_pts.append(p_valid)
            all_cols.append(c_valid)

        # æ¸…ç† scene
        del scene
        del imgs_tensors
        torch.cuda.empty_cache()

        # åˆå¹¶
        if all_pts:
            final_xyz = np.concatenate(all_pts, axis=0)
            final_rgb = np.concatenate(all_cols, axis=0)
            # æ‹¼æŽ¥ xyz å’Œ rgb (Nx6)
            full_cloud = np.hstack([final_xyz, final_rgb.astype(np.float32)])
        else:
            full_cloud = np.zeros((0, 6))

        return np.array(refined_poses), full_cloud

    def _preprocess_image_strict(self, image: np.ndarray, target_h: int, target_w: int):
        import torchvision.transforms.functional as TF
        # [H, W, 3] -> [3, H, W], CPU float
        img_tensor = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
        # Resize
        img_resized = TF.resize(img_tensor, [target_h, target_w], antialias=True)
        # [1, 3, H, W]
        return img_resized.unsqueeze(0)