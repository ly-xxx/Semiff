"""
å·¥ä¸šçº§æŸå¤±å‡½æ•°åº“
æä¾› differentiable çš„å‡ ä½•å¯¹é½æŸå¤±å‡½æ•°
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple


class SoftIoULoss(nn.Module):
    """
    Soft IoU Loss - ç”¨äºäºŒå€¼maskå¯¹é½çš„å¹³æ»‘å¯å¯¼æŸå¤±å‡½æ•°

    IoU = Intersection / Union
    Loss = 1 - IoU

    ç›¸æ¯”MSEçš„ä¼˜åŠ¿ï¼š
    - å¯¹å½¢çŠ¶é‡å æ›´æ•æ„Ÿ
    - æ¢¯åº¦æ›´ç¨³å®š
    - æ•°å­¦æ„ä¹‰æ›´æ˜ç¡®
    """

    def __init__(self, smooth: float = 1e-6):
        """
        Args:
            smooth: å¹³æ»‘é¡¹ï¼Œé˜²æ­¢é™¤é›¶é”™è¯¯
        """
        super(SoftIoULoss, self).__init__()
        self.smooth = smooth

    def forward(self, pred_mask: torch.Tensor, gt_mask: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—Soft IoUæŸå¤±

        Args:
            pred_mask: é¢„æµ‹mask (B, H, W) æˆ– (B, 1, H, W), range [0, 1], differentiable
            gt_mask: çœŸå®mask (B, H, W) æˆ– (B, 1, H, W), range {0, 1}

        Returns:
            IoUæŸå¤±å€¼ (æ ‡é‡)
        """
        # è¾“å…¥éªŒè¯å’Œæ ‡å‡†åŒ–
        if pred_mask.shape != gt_mask.shape:
            raise ValueError(f"Shape mismatch: pred {pred_mask.shape} vs gt {gt_mask.shape}")

        # ç§»é™¤å•é€šé“ç»´åº¦å¦‚æœå­˜åœ¨
        if pred_mask.dim() == 4 and pred_mask.shape[1] == 1:
            pred_mask = pred_mask.squeeze(1)
            gt_mask = gt_mask.squeeze(1)

        if pred_mask.dim() != 3:
            raise ValueError(f"Expected 3D tensors (B, H, W), got {pred_mask.dim()}D")

        # ç¡®ä¿è¾“å…¥èŒƒå›´åˆç†
        pred_mask = torch.clamp(pred_mask, 0, 1)

        # Flattenåˆ°æ‰¹æ¬¡çº§åˆ«è¿›è¡Œè®¡ç®—
        pred_flat = pred_mask.view(pred_mask.size(0), -1)  # (B, H*W)
        gt_flat = gt_mask.view(gt_mask.size(0), -1)        # (B, H*W)

        # è®¡ç®—IoU
        intersection = (pred_flat * gt_flat).sum(dim=1)     # (B,)
        total = (pred_flat + gt_flat).sum(dim=1)            # (B,)
        union = total - intersection                         # (B,)

        # IoU = I / U
        iou = (intersection + self.smooth) / (union + self.smooth)  # (B,)

        # Loss = 1 - IoU (å¹³å‡å€¼)
        loss = 1.0 - iou.mean()

        return loss

    def compute_iou(self, pred_mask: torch.Tensor, gt_mask: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—IoUæŒ‡æ ‡ï¼ˆç”¨äºè¯„ä¼°ï¼Œä¸ç”¨äºè®­ç»ƒï¼‰

        Returns:
            IoUå€¼ (0-1ä¹‹é—´)
        """
        with torch.no_grad():
            # ä½¿ç”¨ç›¸åŒçš„è®¡ç®—é€»è¾‘ä½†ä¸è®¡ç®—æ¢¯åº¦
            pred_flat = pred_mask.view(pred_mask.size(0), -1)
            gt_flat = gt_mask.view(gt_mask.size(0), -1)

            intersection = (pred_flat * gt_flat).sum(dim=1)
            total = (pred_flat + gt_flat).sum(dim=1)
            union = total - intersection

            iou = (intersection + self.smooth) / (union + self.smooth)
            return iou.mean()


class DiceLoss(nn.Module):
    """
    Dice Loss - å¦ä¸€ç§å¸¸ç”¨çš„åˆ†å‰²æŸå¤±å‡½æ•°
    Dice = 2 * Intersection / (|pred| + |gt|)
    Loss = 1 - Dice
    """

    def __init__(self, smooth: float = 1e-6):
        super(DiceLoss, self).__init__()
        self.smooth = smooth

    def forward(self, pred_mask: torch.Tensor, gt_mask: torch.Tensor) -> torch.Tensor:
        pred_flat = pred_mask.view(pred_mask.size(0), -1)
        gt_flat = gt_mask.view(gt_mask.size(0), -1)

        intersection = (pred_flat * gt_flat).sum(dim=1)
        pred_sum = pred_flat.sum(dim=1)
        gt_sum = gt_flat.sum(dim=1)

        dice = (2. * intersection + self.smooth) / (pred_sum + gt_sum + self.smooth)
        loss = 1. - dice.mean()

        return loss


class FocalLoss(nn.Module):
    """
    Focal Loss - è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
    é€‚ç”¨äºå‰æ™¯åƒç´ è¿œå°‘äºèƒŒæ™¯åƒç´ çš„æƒ…å†µ
    """

    def __init__(self, alpha: float = 0.25, gamma: float = 2.0):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma

    def forward(self, pred_mask: torch.Tensor, gt_mask: torch.Tensor) -> torch.Tensor:
        # BCE loss
        bce = F.binary_cross_entropy(pred_mask, gt_mask, reduction='none')

        # Focal modulation
        pt = torch.exp(-bce)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce

        return focal_loss.mean()


def get_loss_function(loss_type: str, **kwargs) -> nn.Module:
    """
    å·¥å‚å‡½æ•°ï¼šæ ¹æ®é…ç½®åˆ›å»ºæŸå¤±å‡½æ•°

    Args:
        loss_type: æŸå¤±å‡½æ•°ç±»å‹ ("soft_iou", "dice", "focal")
        **kwargs: æŸå¤±å‡½æ•°å‚æ•°

    Returns:
        æŸå¤±å‡½æ•°å®ä¾‹
    """
    if loss_type == "soft_iou":
        return SoftIoULoss(**kwargs)
    elif loss_type == "dice":
        return DiceLoss(**kwargs)
    elif loss_type == "focal":
        return FocalLoss(**kwargs)
    else:
        raise ValueError(f"Unknown loss type: {loss_type}")


# å•å…ƒæµ‹è¯•ç¤ºä¾‹
if __name__ == "__main__":
    print("ğŸ§ª Testing Loss Functions...")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    pred = torch.rand(2, 100, 100, requires_grad=True)
    gt = torch.randint(0, 2, (2, 100, 100)).float()

    # æµ‹è¯•SoftIoULoss
    loss_fn = SoftIoULoss()
    loss = loss_fn(pred, gt)
    loss.backward()

    print(".4f")
    print(f"  Gradient Check: {not torch.isnan(pred.grad).any()}")

    # æµ‹è¯•IoUè®¡ç®—
    iou_val = loss_fn.compute_iou(pred.detach(), gt)
    print(".4f")
    # æµ‹è¯•DiceLoss
    dice_loss = DiceLoss()
    dice_val = dice_loss(pred.detach(), gt)
    print(".4f")
    print("âœ… All loss functions working correctly!")
