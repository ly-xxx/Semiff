"""
SAM 2 Wrapper: Video Segmentation with Auto-Prompting (Multi-Object Support)
"""

import torch
import numpy as np
from typing import List, Dict, Any, Optional, Generator, Tuple
from pathlib import Path
import cv2
import os
import matplotlib
import matplotlib.pyplot as plt
import subprocess
import json
import logging
import sys

# å¯¼å…¥ç»Ÿä¸€è·¯å¾„ç®¡ç†å·¥å…·
try:
    from semiff.core.workspace import WorkspaceManager
except ImportError:
    # Fallback: å¦‚æžœå¯¼å…¥å¤±è´¥ï¼Œæ·»åŠ  src åˆ°è·¯å¾„
    _current_file = Path(__file__).resolve()
    _src_dir = _current_file.parents[2]  # src/
    if str(_src_dir) not in sys.path:
        sys.path.insert(0, str(_src_dir))
    from semiff.core.workspace import WorkspaceManager

logger = logging.getLogger("SAM2Wrapper")

class SAM2Wrapper:
    def __init__(self, config: Dict):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        get_cfg = lambda k, d: config.get(k, d) if hasattr(config, "get") else getattr(config, k, d)
        
        # ä»Ž sam2 å­é…ç½®ä¸­è¯»å–å‚æ•°
        sam2_cfg = get_cfg("sam2", {})
        get_sam2_cfg = lambda k, d: sam2_cfg.get(k, d) if hasattr(sam2_cfg, "get") else getattr(sam2_cfg, k, d)
        
        # ðŸ”§ ä½¿ç”¨ç»Ÿä¸€è·¯å¾„è§£æžå·¥å…·
        checkpoint_rel = get_sam2_cfg("checkpoint", "checkpoints/sam2_hiera_large.pt")
        model_cfg_rel = get_sam2_cfg("model_cfg", "configs/sam2.1/sam2.1_hiera_l.yaml")
        
        self.checkpoint = str(WorkspaceManager.resolve_path(checkpoint_rel))
        self.model_cfg = str(WorkspaceManager.resolve_path(model_cfg_rel))
        
        logger.info(f"ðŸ” Resolved checkpoint: {self.checkpoint}")
        logger.info(f"ðŸ” Resolved model_cfg: {self.model_cfg}")

        pipeline_cfg = get_cfg("pipeline", {})
        get_p_cfg = lambda k, d: pipeline_cfg.get(k, d) if hasattr(pipeline_cfg, "get") else getattr(pipeline_cfg, k, d)
        
        self.interactive_mode = get_p_cfg("interactive_mode", False)
        self.input_rotate_code = get_p_cfg("input_rotate_code", None)

        self.predictor = self._init_model()

    def _init_model(self):
        try:
            import sam2
            from hydra.core.global_hydra import GlobalHydra
            from hydra import initialize_config_module, initialize_config_dir

            if GlobalHydra.instance().is_initialized():
                GlobalHydra.instance().clear()

            from sam2.build_sam import build_sam2_video_predictor
            
            if not os.path.exists(self.checkpoint):
                logger.error(f"âŒ Checkpoint not found: {self.checkpoint}")
                return None

            logger.info(f"Loading SAM 2 model...")
            
            if os.path.exists(self.model_cfg):
                abs_config_path = os.path.abspath(self.model_cfg)
                config_dir = os.path.dirname(abs_config_path)
                config_name = os.path.basename(abs_config_path)
                initialize_config_dir(config_dir=config_dir, version_base="1.2")
                return build_sam2_video_predictor(config_name, self.checkpoint, device=self.device)
            else:
                fallback_name = os.path.basename(self.model_cfg).replace("sam2.1", "sam2")
                logger.warning(f"ðŸ”„ Falling back to package config: {fallback_name}")
                initialize_config_module("sam2", version_base="1.2")
                return build_sam2_video_predictor(fallback_name, self.checkpoint, device=self.device)

        except ImportError:
            logger.error("âŒ SAM 2 library not installed.")
            return None
        except Exception as e:
            logger.error(f"âŒ SAM 2 init failed: {e}")
            return None

    def _detect_video_rotation(self, video_path: str) -> Optional[int]:
        try:
            cmd = [
                'ffprobe', '-v', 'quiet', '-print_format', 'json',
                '-show_streams', str(video_path)
            ]
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode != 0: return None

            data = json.loads(result.stdout)
            video_stream = next((s for s in data.get('streams', []) if s['codec_type'] == 'video'), None)
            if not video_stream: return None

            tags = video_stream.get('tags', {})
            rotate = int(tags.get('rotate', 0))

            if rotate == 90: return cv2.ROTATE_90_CLOCKWISE
            elif rotate == 180: return cv2.ROTATE_180
            elif rotate == 270: return cv2.ROTATE_90_COUNTERCLOCKWISE
            elif rotate == -90: return cv2.ROTATE_90_COUNTERCLOCKWISE
            return None
        except Exception:
            return None

    def _inverse_rotate_points(self, points: np.ndarray, h_vis: int, w_vis: int, rotate_code: Optional[int]) -> np.ndarray:
        """
        å°†å¯è§†åŒ–åæ ‡æ˜ å°„å›ž Raw è§†é¢‘åæ ‡ã€‚
        ä¿®æ­£é€»è¾‘ï¼šå¦‚æžœ Raw å›¾åƒå·²ç»è¢« OpenCV è‡ªåŠ¨æ—‹è½¬ï¼ˆå³é•¿å®½æ¯”å’Œ Vis ä¸€è‡´ï¼‰ï¼Œåˆ™ç›´æŽ¥ä½¿ç”¨åŽŸåæ ‡ã€‚
        """
        # 1. è‡ªåŠ¨æ£€æµ‹æ˜¯å¦éœ€è¦æ—‹è½¬
        # æˆ‘ä»¬æ— æ³•ç›´æŽ¥åœ¨è¿™é‡ŒèŽ·å– Raw å°ºå¯¸ï¼Œä½†æˆ‘ä»¬å¯ä»¥é€šè¿‡ rotate_code æŽ¨æ–­ã€‚
        # å¦‚æžœ rotate_code æ˜¯ 90åº¦ï¼Œé€šå¸¸æ„å‘³ç€ Raw æ˜¯æ¨ªå±ï¼ŒVis æ˜¯ç«–å±ã€‚
        # ä½†å¦‚æžœ OpenCV å·²ç»è‡ªåŠ¨æ—‹è½¬äº†ï¼Œé‚£ä¹ˆæˆ‘ä»¬åœ¨å¤–éƒ¨çœ‹åˆ°çš„ frame.shape å·²ç»æ˜¯ç«–å±äº†ã€‚

        # æœ€ç¨³å¦¥çš„æ–¹å¼ï¼šç›´æŽ¥è¿”å›ž pointsã€‚
        # å› ä¸ºåœ¨ run_generator ä¸­ï¼Œæˆ‘ä»¬æ˜¯è¿™æ ·èŽ·å– interactive prompt çš„ï¼š
        # prompts_dict = self._get_interactive_prompt(first_frame, ...)
        # è¿™é‡Œçš„ first_frame æ˜¯ä»Ž cap.read() è¯»å‡ºæ¥çš„ã€‚
        # å¦‚æžœ cap.read() è¯»å‡ºæ¥çš„æ˜¯ç«–å±ï¼ˆæ­£å¦‚ä½ çš„ debug å›¾æ‰€ç¤ºï¼‰ï¼Œ
        # é‚£ä¹ˆç‚¹å‡»åæ ‡(Vis) å’Œ SAM2 è¾“å…¥åæ ‡(Raw) å°±æ˜¯åŒä¸€ä¸ªåæ ‡ç³»ï¼

        return points

        # ä¸‹é¢çš„æ—§ä»£ç å…¨éƒ¨æ³¨é‡ŠæŽ‰æˆ–åˆ é™¤ï¼Œå› ä¸ºä½ çš„ OpenCV çŽ¯å¢ƒä¼šè‡ªåŠ¨å¤„ç†æ—‹è½¬
        """
        if rotate_code is None:
            return points

        new_pts = points.copy()

        # ... (æ—§çš„å¤æ‚æ—‹è½¬é€»è¾‘) ...

        return new_pts
        """

    def _get_interactive_prompt(self, frame: np.ndarray, output_dir: Path, rotate_code: Optional[int] = None) -> Dict[int, np.ndarray]:
        collected_points = {1: [], 2: []}
        
        if os.environ.get('DISPLAY', '') == '':
            logger.warning("âš ï¸ No DISPLAY detected. Using center point default.")
            h, w = frame.shape[:2]
            return {1: np.array([[w // 2, h // 2]], dtype=np.float32)}

        try:
            matplotlib.use('TkAgg')
        except:
            pass
            
        logger.info("\n>>> INTERACTIVE MODE <<<\n[Left Click]: Object (Red)\n[Right Click]: Robot (Blue)\n[Close Window]: Start Segmentation\n")
        
        # 1. ç”Ÿæˆå¯è§†åŒ–å¸§ (ç«–å±)
        display_frame = frame.copy()
        if rotate_code is not None:
            display_frame = cv2.rotate(display_frame, rotate_code)
            
        rgb_display = cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB)
        h_disp, w_disp = display_frame.shape[:2]
        
        fig, ax = plt.subplots(figsize=(10, 8))
        ax.imshow(rgb_display)
        ax.set_title(f"Click Objects (Rot: {rotate_code})")
        ax.axis('off')

        def on_click(event):
            if event.xdata is None or event.ydata is None: return
            if event.inaxes != ax: return

            click_x, click_y = event.xdata, event.ydata
            
            if event.button == 1:
                obj_id = 1
                color = 'r*'
                logger.info(f"ðŸ“ Obj(1) Vis: {int(click_x)}, {int(click_y)}")
            elif event.button == 3:
                obj_id = 2
                color = 'b*'
                logger.info(f"ðŸ¤– Rob(2) Vis: {int(click_x)}, {int(click_y)}")
            else:
                return

            collected_points[obj_id].append([click_x, click_y])
            ax.plot(event.xdata, event.ydata, color, markersize=12)
            fig.canvas.draw()

        fig.canvas.mpl_connect('button_press_event', on_click)
        plt.show(block=True)
        plt.close(fig)
        
        # 2. åæ ‡æ˜ å°„
        result_prompts = {}
        h_raw, w_raw = frame.shape[:2]
        
        # Debug ç”»å¸ƒ (ç«–å± Vis) - éªŒè¯ä½ çš„ç‚¹å‡»
        debug_vis_check = display_frame.copy()

        for obj_id, pts in collected_points.items():
            if not pts: continue
            pts_np = np.array(pts, dtype=np.float32)
            
            # === æ‰§è¡Œæ˜ å°„ ===
            raw_pts = self._inverse_rotate_points(pts_np, h_disp, w_disp, rotate_code)
            
            # === è¶Šç•Œä¿æŠ¤ (Clip to Raw Dimensions) ===
            raw_pts[:, 0] = np.clip(raw_pts[:, 0], 0, w_raw - 1)
            raw_pts[:, 1] = np.clip(raw_pts[:, 1], 0, h_raw - 1)
            
            result_prompts[obj_id] = raw_pts
            
            # åœ¨ Debug å›¾ä¸Šç”»ç‚¹
            color = (0, 0, 255) if obj_id == 1 else (255, 0, 0)
            for (vx, vy) in pts:
                vx_i = int(np.clip(vx, 0, w_disp-1))
                vy_i = int(np.clip(vy, 0, h_disp-1))
                cv2.drawMarker(debug_vis_check, (vx_i, vy_i), color, markerType=cv2.MARKER_STAR, markerSize=30, thickness=3)

        # 3. ä¿å­˜ Debug å›¾
        if output_dir:
            if not output_dir.exists(): output_dir.mkdir(parents=True, exist_ok=True)
            debug_save_path = output_dir / "debug_vis_checks.jpg"
            cv2.imwrite(str(debug_save_path), debug_vis_check)
            print(f"âœ… [DEBUG] Saved: {debug_save_path}")

        # === æ–°å¢žï¼šRaw åæ ‡éªŒè¯ (True Verification) ===
        # è¿™å¼ å›¾æ˜¾ç¤ºçš„æ˜¯ä¼ ç»™ SAM2 çš„çœŸå®žåæ ‡æ˜¯å¦è½åœ¨äº†æ¨ªå±å›¾åƒçš„æ­£ç¡®ç‰©ä½“ä¸Š
        debug_raw_check = frame.copy() # è¿™æ˜¯åŽŸå§‹ Raw æ¨ªå±å›¾
        h_raw, w_raw = frame.shape[:2]

        for obj_id, raw_pts in result_prompts.items():
            color = (0, 0, 255) if obj_id == 1 else (255, 0, 0)
            for (rx, ry) in raw_pts:
                rx_i = int(np.clip(rx, 0, w_raw-1))
                ry_i = int(np.clip(ry, 0, h_raw-1))
                # ç”»ä¸€ä¸ªå¤§å‰ï¼Œç¡®ä¿çœ‹æ¸…
                cv2.drawMarker(debug_raw_check, (rx_i, ry_i), color, markerType=cv2.MARKER_CROSS, markerSize=30, thickness=3)

        if output_dir:
            raw_debug_path = output_dir / "debug_RAW_checks.jpg"
            cv2.imwrite(str(raw_debug_path), debug_raw_check)
            print(f"âœ… [DEBUG] Saved RAW Verification: {raw_debug_path} (Check THIS one!)")

        return result_prompts if result_prompts else None

    def run_generator(self, video_path: str, output_dir: Optional[Path] = None) -> Generator[Dict, None, None]:
        if self.predictor is None:
            logger.error("âŒ Predictor is None. Cannot run.")
            return

        # output_dir å¿…é¡»ç”±è°ƒç”¨è€…æä¾›ï¼ˆå·¥ä½œåŒºç›®å½•ï¼‰ï¼Œä¸ä½¿ç”¨å›žé€€é€»è¾‘
        if output_dir is None:
            raise ValueError("output_dir is required and must be set to workspace directory")
        
        if not output_dir.exists(): 
            output_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"Initializing SAM 2 state with {video_path}...")
        inference_state = self.predictor.init_state(video_path=video_path)
        
        cap = cv2.VideoCapture(video_path)
        ret, first_frame = cap.read()
        cap.release()

        if not ret: raise RuntimeError(f"Cannot read video file: {video_path}")

        effective_rotate_code = self.input_rotate_code
        if effective_rotate_code is None:
            effective_rotate_code = self._detect_video_rotation(video_path)

        prompts_dict = {}
        if self.interactive_mode:
            prompts_dict = self._get_interactive_prompt(first_frame, output_dir, effective_rotate_code)
            if prompts_dict is None:
                yield {"status": "cancelled"}
                return
        else:
            h, w = first_frame.shape[:2]
            prompts_dict = {1: np.array([[w // 2, h // 2]], dtype=np.float32)}

        for obj_id, points in prompts_dict.items():
            labels = np.array([1] * len(points), dtype=np.int32)
            self.predictor.add_new_points_or_box(
                inference_state=inference_state,
                frame_idx=0,
                obj_id=obj_id,
                points=points,
                labels=labels,
            )

        for out_frame_idx, out_obj_ids, out_mask_logits in self.predictor.propagate_in_video(inference_state):
            frame_masks = {}
            for i, obj_id in enumerate(out_obj_ids):
                mask_tensor = (out_mask_logits[i] > 0.0)
                mask_np = mask_tensor.cpu().numpy().squeeze()
                frame_masks[obj_id] = mask_np

            yield {
                "status": "running",
                "frame_idx": out_frame_idx,
                "masks": frame_masks
            }