"""
src/semiff/solvers/aligner.py
æ··åˆå¼è‡ªæ ‡å®šæ ¸å¿ƒæ¨¡å—ï¼šç»“åˆ 3D RANSAC ç²—é…å‡†ä¸ 2D Differentiable Rendering ç²¾é…å‡†
"""

import torch
import torch.nn as nn
import numpy as np
import pytorch_kinematics as pk
import trimesh
import logging
import os
from omegaconf import DictConfig

from semiff.engine.render import DifferentiableRasterizer
from semiff.engine.math_utils import rotation_6d_to_matrix, transform_points, chamfer_distance

# Try to import open3d, fallback if not available
try:
    import open3d as o3d
    HAS_OPEN3D = True
except ImportError:
    HAS_OPEN3D = False
    logging.warning("Open3D not available. RANSAC coarse alignment will be disabled.")

logger = logging.getLogger("ALIGNER")

class HybridAligner(nn.Module):
    def __init__(self, urdf_path, align_cfg: DictConfig, device='cuda'):
        """
        Args:
            urdf_path: URDF æ–‡ä»¶è·¯å¾„
            align_cfg: å¯¹åº” config.yaml ä¸­çš„ 'calibration' éƒ¨åˆ†
            device: è®¡ç®—è®¾å¤‡
        """
        super().__init__()
        self.device = device
        self.cfg = align_cfg
        self.urdf_path = urdf_path

        # 1. Kinematics Chain
        with open(urdf_path, "rb") as f:
            self.chain = pk.build_chain_from_urdf(f.read()).to(dtype=torch.float32, device=device)
            
        self.joint_names = self.chain.get_joint_parameter_names()
        self.n_dof = len(self.joint_names)

        # 2. Mesh Storage (Loaded later)
        self.link_meshes = {} # {link_name: verts_tensor} - å­˜å‚¨åŸå§‹é¡¶ç‚¹
        self.link_faces = {} # {link_name: faces_tensor} - å­˜å‚¨é¢ç‰‡ï¼ˆç”¨äºæ¸²æŸ“ï¼‰
        self.sampled_indices = {} # {link_name: indices} - å­˜å‚¨é¢„å…ˆéšæœºé‡‡æ ·çš„ç´¢å¼•

        # 3. Parameters (åˆå§‹åŒ–)
        # å¦‚æœæœ‰ Coarse é˜¶æ®µï¼Œè¿™äº›ä¼šè¢«è¦†ç›–ï¼›å¦åˆ™ä½¿ç”¨ Config ä¸­çš„ Initial Guess
        init_trans = self.cfg.initial_guess.get("translate", [0,0,1])
        init_scale = self.cfg.initial_guess.get("scale", 1.0)

        self.log_scale = nn.Parameter(torch.tensor(float(np.log(init_scale)), device=device))
        self.base_trans = nn.Parameter(torch.tensor(init_trans, dtype=torch.float32, device=device))
        self.base_rot_6d = nn.Parameter(torch.tensor([1., 0, 0, 0, 1, 0], device=device))

        # C. å…³èŠ‚è§’åº¦ä¿®æ­£ (Joint Optimization)
        # è¿™æ˜¯ä¸ºäº†è§£å†³ "å…³èŠ‚è§’åº¦ä¸ä¸€è‡´" çš„å…³é”®
        self.delta_q = nn.Parameter(torch.zeros(1, self.n_dof, device=device))

        self.renderer = None 

    def load_meshes(self, urdf_root_dir):
        """
        Parse URDF meshes using trimesh and load to GPU.
        Simplified loader: assumes URDF file names match link names or standard structure.
        """
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„ URDF ç»“æ„ç¼–å†™ Mesh åŠ è½½é€»è¾‘
        # ä¸‹é¢æ˜¯ä¸€ä¸ªé€šç”¨çš„å¯å‘å¼åŠ è½½å™¨
        # å®é™…ä½¿ç”¨ä¸­ï¼Œpytorch_kinematics çš„ chain.get_link_names() å¯ä»¥å¸®åŠ©æˆ‘ä»¬
        logger.info(f"ğŸ”© Loading meshes from {urdf_root_dir}...")
        
        # è§£æ URDF è·å– mesh è·¯å¾„ (å€ŸåŠ© urdfpy æˆ– trimesh.load)
        # è¿™é‡Œä¸ºäº†ä¸å¼•å…¥æ–°ä¾èµ–ï¼Œå‡è®¾æˆ‘ä»¬æ‰‹åŠ¨æ‰«æ mesh æ–‡ä»¶å¤¹
        # å‡è®¾ mesh éƒ½åœ¨ robot/visual/ ä¸‹ï¼Œä¸”æ–‡ä»¶ååŒ…å« link å
        # *è¿™æ˜¯ä¸€ä¸ªéœ€è¦æ ¹æ®ä½ å®é™…æ–‡ä»¶ç»“æ„è°ƒæ•´çš„åœ°æ–¹*
        
        # å¤‡é€‰æ–¹æ¡ˆ: ä½¿ç”¨ trimesh åŠ è½½æ•´ä¸ª URDF scene
        try:
            scene = trimesh.load(self.urdf_path)
            # trimesh load urdf returns a Scene with geometry attached to nodes
            # We need to map geometry to link names
            if isinstance(scene, trimesh.Scene):
                 for name, geom in scene.geometry.items():
                    # ç®€åŒ–ï¼šå‡è®¾æ‰€æœ‰ geometry éƒ½åˆå¹¶å¤„ç†ï¼Œæˆ–è€…æ ¹æ® name åŒ¹é… link
                    # åœ¨ Step 2 è„šæœ¬ä¸­æˆ‘ä»¬ä¼šå¤„ç†å…·ä½“çš„ Mesh åŠ è½½
                    pass
        except Exception as e:
            logger.warning(f"Trimesh direct load failed: {e}")

    def inject_mesh_data(self, mesh_dict, num_samples_per_link=512):
        """
        æ³¨å…¥ Mesh æ•°æ®ï¼Œå¹¶é¢„å…ˆç”Ÿæˆé‡‡æ ·ç´¢å¼•ä»¥èŠ‚çœæ˜¾å­˜
        mesh_dict: {link_name: trimesh.Trimesh}
        num_samples_per_link: æ¯ä¸ª link é‡‡æ ·çš„ç‚¹æ•°
        """
        for name, mesh in mesh_dict.items():
            if len(mesh.vertices) == 0: continue
            v = torch.from_numpy(mesh.vertices).float().to(self.device)
            f = torch.from_numpy(mesh.faces).int().to(self.device) if len(mesh.faces) > 0 else None
            
            # éšæœºä¸‹é‡‡æ ·ç´¢å¼•ï¼Œä¿ç•™æ¢¯åº¦ä¼ æ’­èƒ½åŠ›
            # æˆ‘ä»¬ä¸å­˜å‚¨é‡‡æ ·åçš„ç‚¹ï¼Œè€Œæ˜¯å­˜å‚¨ç´¢å¼•ï¼Œå› ä¸ºæˆ‘ä»¬è¦å˜æ¢çš„æ˜¯ v
            n_v = v.shape[0]
            if n_v > num_samples_per_link:
                # éšæœºé€‰ç‚¹
                idx = torch.randperm(n_v, device=self.device)[:num_samples_per_link]
            else:
                idx = torch.arange(n_v, device=self.device)
                
            self.link_meshes[name] = v  # åŸå§‹é¡¶ç‚¹
            if f is not None:
                self.link_faces[name] = f  # é¢ç‰‡ï¼ˆç”¨äºæ¸²æŸ“ï¼‰
            self.sampled_indices[name] = idx  # é‡‡æ ·ç´¢å¼•
            
        logger.info(f"âœ… [Aligner] Injected meshes. Optimization will use dynamic sampling.")

    # ================= PHASE 1: 3D Coarse (CPU/Open3D) =================
    
    def run_coarse_alignment(self, visual_ply_path, joint_cfg, filter_mask_fn=None):
        if not self.cfg.coarse.enable:
            logger.info("â­ï¸ [Phase 1] Coarse alignment disabled by config.")
            return None

        logger.info("ğŸ—ï¸ [Phase 1] Starting 3D RANSAC...")
        num_samples = self.cfg.coarse.num_samples_physical
        thresh = self.cfg.coarse.ransac_threshold

        # A. Sample Physical Robot (Target)
        phy_pts = self._sample_physical_robot(joint_cfg, num_samples=num_samples)

        # B. Load Visual Cloud (Source)
        vis_pcd = o3d.io.read_point_cloud(str(visual_ply_path))
        vis_pts = np.asarray(vis_pcd.points)

        # C. Filter Background (Optional but recommended)
        if self.cfg.coarse.visual_filter.enable and filter_mask_fn:
            mask_indices = filter_mask_fn(vis_pts)
            vis_pts = vis_pts[mask_indices]
            logger.info(f"Filtered points: {len(vis_pcd.points)} -> {len(vis_pts)}")

        if len(vis_pts) < 100:
            logger.warning("âš ï¸ Too few visual points for RANSAC. Using raw cloud.")
            vis_pts = np.asarray(vis_pcd.points)

        # D. RANSAC
        src = o3d.geometry.PointCloud()
        src.points = o3d.utility.Vector3dVector(phy_pts) # Physical
        tgt = o3d.geometry.PointCloud()
        tgt.points = o3d.utility.Vector3dVector(vis_pts) # Visual (MASt3R)
        
        # Note: We want T_sim2real: Physical -> Visual
        # So Physical is Source, Visual is Target
        
        src.estimate_normals()
        tgt.estimate_normals()
        
        # FPFH Features
        voxel_size = thresh * 0.5  # è‡ªé€‚åº”ç‰¹å¾åŠå¾„
        src_fpfh = o3d.pipelines.registration.compute_fpfh_feature(src, o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size*5, max_nn=100))
        tgt_fpfh = o3d.pipelines.registration.compute_fpfh_feature(tgt, o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size*5, max_nn=100))

        result = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(
            src, tgt, src_fpfh, tgt_fpfh,
            mutual_filter=True,
            max_correspondence_distance=thresh,  # ä½¿ç”¨é…ç½®
            estimation_method=o3d.pipelines.registration.TransformationEstimationPointToPoint(with_scaling=True),
            ransac_n=4,
            checkers=[
                o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(0.9),
                o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(thresh)
            ],
            criteria=o3d.pipelines.registration.RANSACConvergenceCriteria(4000000, 500)
        )

        logger.info(f"âœ… RANSAC Fitness: {result.fitness:.4f}")
        self._set_params_from_matrix(result.transformation)
        return result.transformation

    def _sample_physical_robot(self, joint_cfg, num_samples=10000):
        """Use FK to place meshes and sample points"""
        # joint_cfg: dict {name: val}
        # Build tensor batch for FK
        # Note: We need to match joint names. 
        # For simplicity, we assume joint_cfg keys match URDF joint names or we iterate links.
        
        # ç®€å•å¤„ç†ï¼šæˆ‘ä»¬ä¸é€šè¿‡ FK é‡‡æ ·ï¼Œè€Œæ˜¯å‡è®¾ t=0 æ—¶åˆ» Pose å·²çŸ¥
        # æ­£ç¡®åšæ³•ï¼šåˆ©ç”¨ self.chain.forward_kinematics ç®—å‡ºæ¯ä¸ª link çš„ transform
        # ç„¶åæŠŠ self.link_meshes çš„ç‚¹å˜æ¢è¿‡å»
        
        th_q = torch.tensor([list(joint_cfg.values())], device=self.device)
        # joint names matching is crucial here, assuming ordered values for now or robust mapping
        # Ideally: chain.forward_kinematics(th_q) where th_q matches chain.get_joint_parameter_names()
        
        # ä¸´æ—¶ Hack: å¦‚æœ joint_cfg æ˜¯ dict, è½¬æ¢ä¸º list
        joint_names = self.chain.get_joint_parameter_names()
        q_list = [joint_cfg.get(n, 0.0) for n in joint_names]
        th_q = torch.tensor([q_list], device=self.device)

        ret = self.chain.forward_kinematics(th_q)
        
        all_pts = []
        total_verts = sum([len(m[0]) for m in self.link_meshes.values()])
        
        for name, (v_gpu, _) in self.link_meshes.items():
            if name not in ret: continue
            
            # Count proportional samples
            n_samp = int(num_samples * (len(v_gpu) / total_verts))
            if n_samp < 10: n_samp = 10
            
            # Random sample indices
            idx = torch.randperm(len(v_gpu))[:n_samp]
            v_sample = v_gpu[idx] # [N, 3]
            
            # Transform to Base
            trans = ret[name].get_matrix()[0] # [4, 4]
            v_homo = torch.cat([v_sample, torch.ones(len(v_sample), 1, device=self.device)], dim=1)
            v_base = (trans @ v_homo.T).T[:, :3]
            
            all_pts.append(v_base.cpu().numpy())
            
        return np.vstack(all_pts)

    def _set_params_from_matrix(self, T):
        T_ten = torch.tensor(T, dtype=torch.float32, device=self.device)
        scale = torch.norm(T_ten[:3, 0]) # ç®€å•ä¼°ç®—
        self.log_scale.data = torch.log(scale)
        
        R_mat = T_ten[:3, :3] / scale
        # Gram-Schmidt to ensure ortho (RANSAC with scaling might distort)
        u, s, v = torch.svd(R_mat)
        R_ortho = u @ v.T
        
        r6d = R_ortho[:, :2].T.flatten() # 6D representation
        self.base_rot_6d.data = r6d
        self.base_trans.data = T_ten[:3, 3]

    # ================= PHASE 2: 2D Fine (DiffRender) =================

    def get_transform(self):
        """è·å–å˜æ¢å‚æ•°ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
        return self.get_transform_params()
    
    def get_transform_params(self):
        """è·å–å˜æ¢å‚æ•°"""
        s = torch.exp(self.log_scale)
        R = rotation_6d_to_matrix(self.base_rot_6d.unsqueeze(0))[0]
        t = self.base_trans
        return s, R, t

    def get_robot_point_cloud(self, q_current):
        """
        ç”Ÿæˆå½“å‰å‚æ•°ä¸‹çš„æœºå™¨äºº 3D ç‚¹äº‘ (Differentiable)
        
        Args:
            q_current: [1, n_dof] å…³èŠ‚è§’åº¦
        
        Returns:
            cloud_urdf: [1, M, 3] æœºå™¨äººç‚¹äº‘ï¼ˆåœ¨ Robot Base Frame ä¸‹ï¼‰
        """
        # 1. æ­£å‘è¿åŠ¨å­¦ FK
        ret = self.chain.forward_kinematics(q_current)
        
        sampled_points = []
        
        for name, v_all in self.link_meshes.items():
            if name not in ret: continue
            
            # å–å‡ºé¢„é€‰çš„é‡‡æ ·ç‚¹
            idx = self.sampled_indices[name]
            v_batch = v_all[idx]  # [M, 3]
            
            # FK å˜æ¢: Local -> Robot Base Frame
            trans = ret[name].get_matrix()  # [1, 4, 4]
            
            ones = torch.ones(v_batch.shape[0], 1, device=self.device)
            v_homo = torch.cat([v_batch, ones], dim=1).unsqueeze(0)  # [1, M, 4]
            
            # [1, 4, 4] @ [1, M, 4].T -> [1, 4, M] -> [1, M, 4] -> [1, M, 3]
            v_base = torch.bmm(trans, v_homo.transpose(1, 2)).transpose(1, 2)[:, :, :3]
            
            sampled_points.append(v_base)
            
        if not sampled_points:
            return None
            
        return torch.cat(sampled_points, dim=1)  # [1, Total_Points, 3]

    def forward(self, q_init, obs_cloud=None, cam_poses=None, K=None, H=None, W=None):
        """
        å‰å‘ä¼ æ’­ï¼šæ”¯æŒä¸¤ç§æ¨¡å¼
        1. Chamfer Distance æ¨¡å¼ï¼šq_init, obs_cloud ä¸ä¸º None
        2. æ¸²æŸ“æ¨¡å¼ï¼šcam_poses, K, H, W ä¸ä¸º Noneï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
        
        Args:
            q_init: [1, n_dof] åˆå§‹å…³èŠ‚è§’åº¦
            obs_cloud: [1, N, 3] MASt3R è§‚æµ‹ç‚¹äº‘ï¼ˆå·²è¿‡æ»¤èƒŒæ™¯ï¼‰ï¼Œç”¨äº Chamfer Loss
            cam_poses: [B, 4, 4] World-to-Cameraï¼Œç”¨äºæ¸²æŸ“
            K: [B, 3, 3] å†…å‚çŸ©é˜µ
            H, W: å›¾åƒå°ºå¯¸
        """
        # æ¨¡å¼1: Chamfer Distance ä¼˜åŒ–
        if obs_cloud is not None:
            return self._forward_chamfer(q_init, obs_cloud)
        
        # æ¨¡å¼2: æ¸²æŸ“æ¨¡å¼ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
        elif cam_poses is not None and K is not None and H is not None and W is not None:
            return self._forward_render(q_init, cam_poses, K, H, W)
        
        else:
            raise ValueError("Either obs_cloud or (cam_poses, K, H, W) must be provided")
    
    def _forward_chamfer(self, q_init, obs_cloud):
        """
        è®¡ç®— 3D Chamfer Loss
        Args:
            q_init: åˆå§‹å…³èŠ‚è§’åº¦ [1, n_dof]
            obs_cloud: MASt3R è§‚æµ‹ç‚¹äº‘ [1, N, 3] (å·²è¿‡æ»¤èƒŒæ™¯)
        Returns:
            loss: scalar tensor
        """
        # 1. å‡†å¤‡å‚æ•°
        s, R_base, t_base = self.get_transform_params()
        
        # 2. å…³èŠ‚è§’åº¦è‡ªé€‚åº”: q = q_init + delta_q
        q_current = q_init + self.delta_q
        
        # 3. ç”Ÿæˆè™šæ‹Ÿæœºå™¨äººç‚¹äº‘ (Ground Truth Geometry, in Robot Base Frame)
        # P_urdf(q)
        cloud_urdf = self.get_robot_point_cloud(q_current)  # [1, M, 3]
        
        if cloud_urdf is None:
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        # 4. å˜æ¢è§‚æµ‹ç‚¹äº‘ (MASt3R -> Robot Base Frame)
        # æˆ‘ä»¬å¯»æ‰¾ T_base å’Œ s ä½¿å¾—ï¼š T_base * s * P_mast3r ~= P_urdf
        # å…¬å¼: P_obs_aligned = (P_mast3r * s) @ R_base.T + t_base
        # æ³¨æ„ï¼šR_base, t_base æ˜¯ World(MASt3R) -> Robot çš„å˜æ¢
        
        obs_cloud_scaled = obs_cloud * s
        
        # ä½¿ç”¨ R_base, t_base ä½œä¸º World(MASt3R) -> Robot çš„å˜æ¢å‚æ•°
        # R_base æ˜¯ [3, 3]ï¼Œobs_cloud_scaled æ˜¯ [1, N, 3]
        # P_obs_aligned = (P_mast3r * s) @ R_base.T + t_base
        obs_cloud_aligned = torch.matmul(obs_cloud_scaled, R_base.T) + t_base.unsqueeze(0)
        
        # 5. è®¡ç®— Loss
        loss = chamfer_distance(obs_cloud_aligned, cloud_urdf)
        
        return loss
    
    def _forward_render(self, joint_cfg_tensor, cam_poses, K, H, W):
        """
        Render the robot at current estimate pose (ä¿æŒå‘åå…¼å®¹)
        cam_poses: [B, 4, 4] World-to-Camera
        """
        if self.renderer is None:
            self.renderer = DifferentiableRasterizer(H, W, device=self.device)
            
        s, R_sim2real, t_sim2real = self.get_transform_params()
        
        # 1. FK (Batch=1 for static robot)
        ret = self.chain.forward_kinematics(joint_cfg_tensor)
        
        all_verts = []
        all_faces = []
        offset = 0
        
        for name, v_loc in self.link_meshes.items():
            if name not in ret: continue
            tg = ret[name].get_matrix()  # [1, 4, 4]
            
            # Link -> Physical Base (ä½¿ç”¨å…¨éƒ¨é¡¶ç‚¹ï¼Œä¸é‡‡æ ·ï¼Œä»¥ä¿è¯æ¸²æŸ“è´¨é‡)
            ones = torch.ones(len(v_loc), 1, device=self.device)
            v_homo = torch.cat([v_loc, ones], dim=1)
            v_phys = (tg @ v_homo.T).transpose(1, 2)[:, :, :3]  # [1, N, 3]
            
            # Physical Base -> Visual World
            v_vis = (v_phys @ R_sim2real.T + t_sim2real) * s
            
            # Repeat for batch size of cameras
            B = cam_poses.shape[0]
            v_vis_batch = v_vis.repeat(B, 1, 1)  # [B, N, 3]
            
            all_verts.append(v_vis_batch)
            
            # æ·»åŠ  facesï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if name in self.link_faces:
                f_loc = self.link_faces[name]
                all_faces.append(f_loc + offset)
            
            offset += len(v_loc)
            
        if not all_verts: 
            return None
        
        mesh_verts = torch.cat(all_verts, dim=1)  # [B, Total_N, 3] World Space
        
        if not all_faces:
            logger.warning("âš ï¸ No faces available for rendering. Returning None.")
            return None
        
        mesh_faces = torch.cat(all_faces, dim=0)

        # 2. Render (ä¼ é€’ç›¸æœºä½å§¿ï¼Œè®©renderå‡½æ•°å†…éƒ¨å¤„ç†World->Camera->Clipå˜æ¢)
        masks = self.renderer.render(mesh_verts, mesh_faces, K, cam_poses)

        # Debug: æ£€æŸ¥æ¸²æŸ“ç»“æœ
        if masks is not None and torch.rand(1) < 0.01:
            logger.info(f"ğŸ¤– Aligner Debug - Rendered masks shape: {masks.shape}, stats: min={masks.min():.4f}, max={masks.max():.4f}, mean={masks.mean():.4f}")

        return masks